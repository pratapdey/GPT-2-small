{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOvQLP0ieJghBSRGpWOCZpc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Task 1 || GPT-2 Model & Checkpoints"],"metadata":{"id":"LB9bOtNrk6Uc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cnBsSgyMk5Am"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","# Configuration for a small GPT-2 model\n","class GPT2Config:\n","    vocab_size = 100000\n","    max_position_embeddings = 768\n","    n_layers = 12\n","    n_heads = 12\n","    n_embd = 512\n","    layer_norm_epsilon = 1e-5\n","    initializer_range = 0.02\n","\n","config = GPT2Config()\n","\n","# Define the scaled dot product attention function\n","def scaled_dot_product_attention(query, key, value):\n","    temp = query.bmm(key.transpose(1, 2)) / math.sqrt(query.size(-1))\n","    softmax = nn.Softmax(dim=-1)\n","    return softmax(temp).bmm(value)\n","\n","# Define a single head for the Multi-Head Attention\n","class AttentionHead(nn.Module):\n","    def __init__(self, embd_dim):\n","        super().__init__()\n","        self.query = nn.Linear(embd_dim, embd_dim)\n","        self.key = nn.Linear(embd_dim, embd_dim)\n","        self.value = nn.Linear(embd_dim, embd_dim)\n","\n","    def forward(self, hidden_state):\n","        return scaled_dot_product_attention(\n","            self.query(hidden_state), self.key(hidden_state), self.value(hidden_state)\n","        )\n","\n","# Define the Multi-Head Attention layer\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, embd_dim, n_heads):\n","        super().__init__()\n","        self.heads = nn.ModuleList([AttentionHead(embd_dim) for _ in range(n_heads)])\n","        self.linear = nn.Linear(n_heads * embd_dim, embd_dim)\n","\n","    def forward(self, hidden_state):\n","        attention = [head(hidden_state) for head in self.heads]\n","        concatenated = torch.cat(attention, dim=-1)\n","        return self.linear(concatenated)\n","\n","# Define the Pointwise Feed Forward layer\n","class PointwiseFeedForward(nn.Module):\n","    def __init__(self, embd_dim, ff_dim):\n","        super().__init__()\n","        self.linear1 = nn.Linear(embd_dim, ff_dim)\n","        self.linear2 = nn.Linear(ff_dim, embd_dim)\n","\n","    def forward(self, hidden_state):\n","        return self.linear2(F.relu(self.linear1(hidden_state)))\n","\n","# Define a single Transformer block\n","class TransformerBlock(nn.Module):\n","    def __init__(self, embd_dim, n_heads, ff_dim, layer_norm_epsilon):\n","        super().__init__()\n","        self.attention = MultiHeadAttention(embd_dim, n_heads)\n","        self.feed_forward = PointwiseFeedForward(embd_dim, ff_dim)\n","        self.layer_norm1 = nn.LayerNorm(embd_dim, eps=layer_norm_epsilon)\n","        self.layer_norm2 = nn.LayerNorm(embd_dim, eps=layer_norm_epsilon)\n","\n","    def forward(self, hidden_state):\n","        attention_output = self.attention(hidden_state)\n","        norm1 = self.layer_norm1(hidden_state + attention_output)\n","        feed_forward_output = self.feed_forward(norm1)\n","        norm2 = self.layer_norm2(norm1 + feed_forward_output)\n","        return norm2\n","\n","# Define the full GPT-2 model\n","class GPT2(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.embd_dim = config.n_embd\n","        self.token_embedding = nn.Embedding(config.vocab_size, self.embd_dim)\n","        self.position_embedding = nn.Embedding(config.max_position_embeddings, self.embd_dim)\n","        self.blocks = nn.ModuleList(\n","            [TransformerBlock(self.embd_dim, config.n_heads, 4 * self.embd_dim, config.layer_norm_epsilon) for _ in range(config.n_layers)]\n","        )\n","        self.layer_norm = nn.LayerNorm(self.embd_dim, eps=config.layer_norm_epsilon)\n","\n","    def forward(self, input_ids, positions_ids=None):\n","        if positions_ids is None:\n","            positions_ids = torch.arange(0, input_ids.size(1)).unsqueeze(0).to(input_ids.device)\n","        tokens = self.token_embedding(input_ids)\n","        positions = self.position_embedding(positions_ids)\n","\n","        # Add positional encoding\n","        x = tokens + positions\n","\n","        for block in self.blocks:\n","            x = block(x)\n","\n","        x = self.layer_norm(x)\n","        return x\n","\n","# Example usage\n","model = GPT2(config)\n","input_ids = torch.randint(0, config.vocab_size, (1, 768))\n","output = model(input_ids)\n","print(output)\n"]},{"cell_type":"markdown","source":["#Task 2 || Transformer Architectural Changes"],"metadata":{"id":"0Mz78k8KlBUS"}},{"cell_type":"markdown","source":["##Rotary Positional Embedding"],"metadata":{"id":"BfSNucd3lCVY"}},{"cell_type":"code","source":["import torch\n","\n","def Rotary_Positional_Embedding(x, sincos):\n","    sin, cos = map(lambda t: t.repeat_interleave(2, dim=-1), sincos)\n","    return (x * cos) + (torch.roll(x, shifts=1, dims=-1) * sin)"],"metadata":{"id":"foq8QVW6lFgN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Group Query Attention"],"metadata":{"id":"AIx4YXpolIm5"}},{"cell_type":"code","source":["def Group_Query_Attention(query, key, value, num_groups):\n","    # Split queries into groups\n","    group_size = query.size(2) // num_groups\n","    query_groups = query.view(*query.size()[:2], num_groups, group_size)\n","\n","    # Perform attention within each group\n","    attention_output = []\n","    for i in range(num_groups):\n","        group_attn_output = scaled_dot_product_attention(query_groups[:,:,i,:], key, value)\n","        attention_output.append(group_attn_output)\n","\n","    # Concatenate the outputs of each group\n","    return torch.cat(attention_output, dim=-1)"],"metadata":{"id":"zu3VnWoglL6W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Sliding_Window_Attention"],"metadata":{"id":"AcqQrGbDlOWi"}},{"cell_type":"code","source":["def Sliding_Window_Attention(query, key, value, window_size):\n","    # Assume query, key, and value are all the same size for simplicity\n","    batch_size, seq_length, dim = query.size()\n","    attention_scores = torch.empty((batch_size, seq_length, window_size), device=query.device)\n","\n","    # Compute attention scores for a sliding window\n","    for i in range(seq_length):\n","        start = max(0, i - window_size // 2)\n","        end = min(seq_length, i + window_size // 2 + 1)\n","        attention_scores[:, i, :end-start] = torch.bmm(query[:, i:i+1, :], key[:, start:end, :].transpose(1, 2))\n","\n","    # Apply softmax to get attention probabilities\n","    attention_probs = torch.nn.functional.softmax(attention_scores, dim=-1)\n","\n","    # Compute weighted sum to get the attention output\n","    attention_output = torch.bmm(attention_probs, value[:, start:end, :])\n","    return attention_output"],"metadata":{"id":"MPZm5Ie_lSeU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Task 3 || Training Loop Implementation"],"metadata":{"id":"ZYOix9hulWje"}},{"cell_type":"markdown","source":["##Single GPU training loop"],"metadata":{"id":"gkMWR-uylXg8"}},{"cell_type":"code","source":["import torch\n","\n","# Assuming model, dataset, optimizer, and loss function are defined\n","\n","# model = Model()\n","# dataset = Dataset()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n","# loss_function = torch.nn.CrossEntropyLoss()\n","\n","# Check for GPU availability\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Move model to the selected device\n","model.to(device)\n","\n","# Assuming dataloader is defined\n","# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    for batch in dataloader:\n","        inputs, targets = batch\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = loss_function(outputs, targets)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","# print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"],"metadata":{"id":"JhjCvgNzlbIK"},"execution_count":null,"outputs":[]}]}